# 软件缺陷预测目标

1. 排名任务（我们的目标）
   1. 根据代码行数和类响应等指标从软件模块中获取数据
   2. 根据已知缺陷数量的模块数据构建模型
   3. 利用该模型对缺陷数未知的软件模块进行缺陷预测，从而得到这些模块基于预测的缺陷数的一个排序
2. 分类任务

----

# 研究发展过程

1. 线性回归、负二项回归、递归划分、贝叶斯加性回归树、随机森林等，通过最大似然估计或最小二乘来构建缺陷预测模型，侧重于每个样本的拟合--->**噪声数据导致难以预测软件模块中缺陷的准确数量**
2. 最小化平均误差、相对误差或均方误差等预测误差度量来构建模型--->**无法提供对排名性能的直接洞察**
3. 采用进化算法直接优化排名性能、使用排序学习方法和随机森林更好地为排序任务构建缺陷预测模型--->**单目标方法不足以解决问题**
4. 多目标方法

----

# 多目标方法

## 三个目标：良好的排名（FPA）、较高的预测精度（MSE）和简单的模型（NNZ）

## 评价指标

1. **FPA**：前 i (i:1到m) 预测模块中实际缺陷比例的平均值（排名性能度量）

   **更大的FPA**意味着更好的排名表现，并且可以在整体上帮助更好地分配测试资源

2. **MSE**：均方误差（预测精度）

   **更小的MSE**意味着预测精度越高

3. **NNZ**：训练得到的相应参数非零的数量（模型复杂度）

   **更小的NNZ**意味着更稀疏(更简单)的模型

4. **AAE**：平均绝对误差（模型的回归性能）

   **更小的AAZ**意味着模型的回归性能更好

## 算法实现

1. 采用线性模型作为基础预测器
2. 采用非支配排序遗传算法 II (NSGA-II)作为多目标学习算法
3. 目的是优化两个或三个评价指标（FPA、MSE、NNZ）

## 算法优化

​	**通过随机设置部分参数为0来修正NSGA-II，以获得更加稀疏的模型**

----

# 实验设置

​	将多目标学习方法优化的线性模型与其他方法的线性模型进行比较

​	数据集：PROMISE仓库

## 研究问题

1. 与单目标进化算法(排序学习方法)实现的线性模型相比，多目标方法获得的线性模型表现如何?
2. 与其他线性模型(linear regression、lasso regression和ridge regression)相比，我们的线性模型表现如何?

## 对照组

### 1. 单目标进化算法：排序学习方法（learning-to-ranking approach）

​	**直接通过优化排序性能来构建缺陷预测模型**

​	定义：基于机器学习中用于解决分类与回归问题的思想，提出利用机器学习方法解决排序的问题。排序学习的目标在于自动地从训练数据中学习得到一个排序函数，使其在文本检索中能够针对文本的相关性、重要性等衡量标准对文本进行排序。

### 2. 其他线性模型：普通线性回归（linear regression）、套索回归（lasso regression）、岭回归（ridge regression）、随机森林

​	**通过最小化均方误差来构建模型**

1. linear regression：给定数据集D={(x1, y1), (x2, y2), ... }，我们试图从此数据集中学习得到一个线性模型，这个模型尽可能准确地反应x(i)和y(i)的对应关系。这里的线性模型，就是属性(x)的线性组合的函数。

   对连续型数据做出预测，从大量数据中推测出代表数据规律的数学表达式。

   特征独立

   多元线性回归：具有n个特征值，预测值公式中有θ

   [sklearn](https://so.csdn.net/so/search?q=sklearn&spm=1001.2101.3001.7020)中LinearRegression中的fit()方法就是通过训练集求出θ

   通过预测方法获得预测值

   ```
   model.fit(train_X, train_y)//拟合
   model.predict(train_X)//预测
   ```

   再通过预测值和我们选定的评价目标指标计算所有的评价指标值。

2. lasso regression：该方法是一种压缩估计。它通过构造一个惩罚函数得到一个较为精炼的模型，使得它压缩一些回归系数，即强制系数绝对值之和小于某个固定值；同时设定一些回归系数为零。因此保留了子集收缩的优点，是一种处理具有复共线性数据的有偏估计。

   拉格朗日乘数法

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200114113322728.png)

   对于参数w增加一个限定条件，能到达和岭回归一样的效果：

   在lambda足够小的时候，一些系数会因此被迫缩减到0
   比现行回归和岭回归具有更强的缩减系数的能力 w = 0

3. LassoLarsCV（Lasso升级版）

   参数：

   cv：int，交叉验证生成器或迭代器，可选的

   　　　　　　确定交叉验证拆分策略。 cv可能的输入是：

   　　　　　　a）None，使用默认的3折交叉验证，

   　　　　　　b）Integer，以指定折叠的数量。

   　　　　　　c）一个要用作交叉验证生成器的对象。

   　　　　　　d）一个迭代生产的训练/测试分割。

   　　　　　　对于整数/无输入，使用KFold。

   过程：

   fit（X，y）使用X，y作为训练数据来拟合模型。

   　　　　　　get_params（[deep]）获取此估算器的参数。

   Predict（X）使用线性模型进行预测

4. ridge regression：一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。

   缩减系数，岭回归是加了二阶正则项(lambda*I)的最小二乘，主要适用于过拟合严重或各变量之间存在多重共线性的时候

   过拟合：其实就是所建的[机器学习](https://so.csdn.net/so/search?q=机器学习&spm=1001.2101.3001.7020)模型或者是深度学习模型在训练样本中表现得过于优越，导致在验证数据集以及测试数据集中表现不佳。

   欠拟合：就是训练样本集过少，或者是属性过少，或者是属性不重要都有可能导致欠拟合的

   ![image-20220909110217101](http://120.48.12.99:8080/examples/pic/image-20220909110217101.png)

   ![image-20220909110305546](http://120.48.12.99:8080/examples/pic/image-20220909110305546.png)

   验证alpha和系数的关系

   ```
   alpha = list(np.linspace(start=0.1, stop=1000, num=10000))
   # alpha = [0.001, 0.01, 0.1, 1, 5, 10, 15, 20, 25, 30, 50, 100, 300, 500, 1000]
   model = RidgeCV(alphas=alpha)
   ```

   

5. 随机森林：随机森林是一种由决策树构成的（并行）集成算法，属于Bagging类型，通过组合多个弱分类器，最终结果通过投票或取均值，使得整体模型的结果具有较高的精确度和泛化性能，同时也有很好的稳定性。

## 实现

1. 采用跨版本缺陷预测
2. **遗传进化算法工具箱GeatPy2实现了两种多目标算法：NSGA-II和改进的NSGA-II**
3. **scikit-learn实现线性回归、套索回归和岭回归**

## 实验结果

​	整体而言，与其他线性模型相比，修正后的NSGA-II可以构建稀疏的线性模型。此外，改进的多目标方法在大多数数据集上的表现与原多目标方法和其他方法（FPA）相当。 在某些情况下，他们甚至取得了更好的结果。根据 MSE，修正后的多目标方法比原始的多目标方法和排序学习方法表现更好，表现与线性回归、lasso回归和ridge回归一样好。这说明改进后的多目标方法具有较好的性能。

----

# 神经网络

## NN



神经网络

![mlp](http://120.48.12.99:8080/examples/pic/mlp.png)

训练就是训练NN每一层的权重。

神经网络是由多个神经元组成的拓扑结构，由多个层排列组成，每一层又堆叠了多个神经元。通常包括输入层，N个隐藏层，和输出层组成。

正向传播

```
self.hidden_opt = tf.matmul(self.X, self.weight1) + self.bias1 # 输入层到隐藏层正向传播
self.hidden_opt = tf.nn.sigmoid(self.hidden_opt)  # 激活函数（非线性函数），得到神经元的活性值，用于计算节点输出值
self.final_opt = tf.matmul(self.hidden_opt, self.weight2) + self.bias2 # 隐藏层到输出层正向传播
self.final_opt = tf.nn.relu(self.final_opt)
```

**神经网络的本质是可以对输入特征进行任意复杂的非线性变换转换为易分类特征**

## BPNN

```
梯度下降算法，这里使用了反向传播算法用于修改权重，减小损失
反向传播神经网络
```

存在问题：误差经 过每一层传递都会不断衰减．当网络层数很深时，梯度就会不停衰减，甚至消 失，使得整个网络很难训练．这就是所谓的梯度消失问题（Vanishing Gradient Problem），也称为梯度弥散问题．

BP算法：

通过BP算法对之前的权重w1 2 3 进行更新推导

（2）构建损失函数，参数学习

完成了第一步，有了模型，下一步就是构建用于优化的损失函数，利用损失函数来优化出网络中所有的待求参数w和b

常用的损失函数使用过的是交叉熵损失函数，求其最小化


 yt对应真实标签，y_是模型预测的结果

（3）有了损失函数就可以根据实际的需要选择合适的优化器迭代优化上面的损失函数

（4）在实战应用中最后要构建模型评价指标如测试准确率，来监控模型训练的状态

----

# 绘制的结果各种结果图

## nsga2优化算法与线性回归对比

![ant-1.3_ant-1.4](http://120.48.12.99:8080/examples/pic/ant-1.3_ant-1.4.png)

## 单目标和多目标对比

![ant-1.4](http://120.48.12.99:8080/examples/pic/ant-1.4.png)

## 多目标和多目标对比

![ant-1.4](http://120.48.12.99:8080/examples/pic/ant-1.41.png)

![ant-1.3_ant-1.4](http://120.48.12.99:8080/examples/pic/ant-1.3_ant-1.40.png)

## 单目标、多目标、改进后的多目标对比

![ant-1.3_ant-1.4](http://120.48.12.99:8080/examples/pic/ant-1.3_ant-1.41.png)

## 多目标和随机森林对比

![ant-1.3_ant-1.4](http://120.48.12.99:8080/examples/pic/ant-1.3_ant-1.42.png)

## 多目标不同评价指标对比

![ant-1.4](http://120.48.12.99:8080/examples/pic/ant-1.42.png)

----

# 补充

## NSGA算法

​	NSGA算法是以遗传算法为基础并基于Pareto最优概念得到的。NSGA算法与基本遗传算法的主要区别是其在进行选择操作之前对个体进行了快速非支配排序，增大了优秀个体被保留的概率，而选择、交叉、变异等操作与基本遗传算法无异。

## NSGA-II算法相比于NSGA

1. NSGA-II算法使用了快速非支配排序法，将算法的计算复杂度由*O*(*mN3*)降到了*O*(*mN2*)，使得算法的计算时间大大减少。
2. 采用了精英策略，将父代个体与子代个体合并后进行非支配排序，使得搜索空间变大，生成下一代父代种群时按顺序将优先级较高的个体选入，并在同级个体中采用拥挤度进行选择，保证了优秀个体能够有更大的概率被保留。
3. 用拥挤度的方法代替了需指定共享半径的适应度共享策略，并作为在同级个体中选择优秀个体的标准，保证了种群中个体的多样性，有利于个体能够在整个区间内进行选择、交叉和变异。

## Pareto最优概念

​	帕累托最优（Pareto Optimality），也称为帕累托效率（Pareto efficiency），是指资源分配的一种理想状态，假定固有的一群人和可分配的资源，从一种分配状态到另一种状态的变化中，在没有使任何人境况变坏的前提下，使得至少一个人变得更好，这就是帕累托改进或帕累托最优化。

## 进化算法

​	进化算法(Evolutionary Algorithm, EA)是一类通过模拟自然界生物自然选择和自然进化的随机搜索算法。自然界生物在周而复始的繁衍中，基因的重组、变异等，使其不断具有新的性状，以适应复杂多变的环境，从而实现进化。进化算法精简了这种复杂的过程而抽象出一套数学模型，用较为简单的编码方式来表现复杂的现象，并通过简化的遗传过程来实现对复杂搜索空间的启发式搜索，最终能够在较大的概率下找到全局最优解，同时天然地支持并行计算。

​	在许多实际问题中，我们常常要处理的数学模型不止有一个目标函数。例如在产品加工与配送系统中，通常要求加工和配送的成本尽可能低，而所花的时间尽可能少，从而使总利润最大。有些多目标优化问题中各个目标之间会有冲突，无法同时取得最优，例如工人的工资和企业的总利润。

​	以遗传算法为代表的许多进化算法，具有生成多个点并进行多方向搜索的特征，因此非常适合求解这种最优解的搜索空间非常复杂的多目标优化问题。（http://geatpy.com/index.php/2019/07/28/第七章：多目标优化/）

## GeatPy

### 四个大类：

1. **Algorithm(算法模板顶级父类)**

   Algorithm类是进化算法的核心类。它既存储着跟进化算法相关的一些参数，同时也在其继承类中实现具体的进化算法。

2. **Population(种群类)**

   Population类是一个表示种群的类。一个种群包含很多个个体，而每个个体都有一条染色体(若要用多染色体，则使用PsyPopulation类)。

3. PsyPopulation(多染色体种群类)

   PsyPopulation类是继承了Population的支持多染色体混合编码的种群类。

4. **Problem(问题类)**

   Problem类定义了与问题相关的一些信息，如问题名称name、优化目标的维数M、决策变量的个数Dim、决策变量的范围ranges、决策变量的边界borders等。

-----

# 特征选择

![](http://120.48.12.99:8080/examples/特征选择.png)